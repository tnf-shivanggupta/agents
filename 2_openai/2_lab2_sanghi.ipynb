{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install agents sendgrid python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, trace, function_tool, OpenAIChatCompletionsModel\n",
    "import os\n",
    "from sendgrid.helpers.mail import Mail, Email, To, Content\n",
    "from openai import AsyncOpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "MODEL = 'llama-3.3-70b-versatile'\n",
    "GROQ_BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")  # Add your Groq API key to your .env file\n",
    "\n",
    "# Point OpenAI client to Groq endpoint\n",
    "groq_client = AsyncOpenAI(\n",
    "    api_key=groq_api_key,\n",
    "    base_url=GROQ_BASE_URL\n",
    ")\n",
    "\n",
    "groq_model = OpenAIChatCompletionsModel(model=MODEL, openai_client=groq_client)\n",
    "\n",
    "\n",
    "def createAgent(name,instructions, tools = [], handoffs = [], handoff_description = \"If you are unable to help, handoff to another agent.\"):\n",
    "    agent = Agent(\n",
    "        name=name,\n",
    "        instructions=instructions,\n",
    "        tools=tools,\n",
    "        handoffs=handoffs,\n",
    "        handoff_description=handoff_description,\n",
    "        model=groq_model,\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Agent workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mathTeacherInstructions = \"You are a maths teacher, \\\n",
    "You job is to help students with their maths homework. \\\n",
    "You are patient and explain concepts clearly. \\\n",
    "You provide step by step solutions to problems.\"\n",
    "\n",
    "translatorInstructions = \"You are a translator, \\\n",
    "Translate the given text to the specified language. \\\n",
    "You are fluent in multiple languages and understand cultural nuances.\"\n",
    "\n",
    "controllerInstructions = \"You are a controller agent, \\\n",
    "You decide which agent to call based on the user's request. \\\n",
    "You can call the maths agent for math related questions and the translator agent for translation tasks. \\\n",
    "If the request does not match any, politely say: \\\n",
    "I'm not the right agent for this request.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "maths_agent = createAgent(\"Professional Maths Teacher\",mathTeacherInstructions)\n",
    "translator_agent = createAgent(\"Translator\",translatorInstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "maths_tools = maths_agent.as_tool(tool_name=\"Maths Tool\", tool_description=\"Helps with math problems and explanations.\")\n",
    "translator_tools = translator_agent.as_tool(tool_name=\"Translator Tool\", tool_description=\"Translates text to specified languages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller_agent = createAgent(\"Controller Agent\", controllerInstructions, \n",
    "                                tools = [],\n",
    "                                handoffs=[maths_agent, translator_agent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professional Maths Teacher\n",
      "I hope this explanation helped you understand how to solve the equation. Do you have any questions or would you like to practice with more examples?\n",
      "Controller Agent\n",
      "I'm not the right agent for this request.\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"tool call validation failed: attempted to call tool 'Translator Tool_args' which was not in request.tools\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=Translator Tool_args>{\"input\": \"Hello, how are you?\"}</function>'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.last_agent.name)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.final_output)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(controller_agent, \u001b[33m\"\u001b[39m\u001b[33mCan you translate \u001b[39m\u001b[33m'\u001b[39m\u001b[33mHello, how are you?\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to hindi?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.last_agent.name)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.final_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agents/.venv/lib/python3.12/site-packages/agents/run.py:267\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, conversation_id, session)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\u001b[39;00m\n\u001b[32m    234\u001b[39m \u001b[33;03moutput is generated. The loop runs like so:\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[33;03m1. The agent is invoked with the given input.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m \u001b[33;03m    agent. Agents may perform handoffs, so we don't know the specific type of the output.\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    266\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    268\u001b[39m     starting_agent,\n\u001b[32m    269\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    270\u001b[39m     context=context,\n\u001b[32m    271\u001b[39m     max_turns=max_turns,\n\u001b[32m    272\u001b[39m     hooks=hooks,\n\u001b[32m    273\u001b[39m     run_config=run_config,\n\u001b[32m    274\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    275\u001b[39m     conversation_id=conversation_id,\n\u001b[32m    276\u001b[39m     session=session,\n\u001b[32m    277\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agents/.venv/lib/python3.12/site-packages/agents/run.py:481\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m logger.debug(\n\u001b[32m    477\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    478\u001b[39m )\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    482\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    483\u001b[39m             starting_agent,\n\u001b[32m    484\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    485\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    486\u001b[39m             _copy_str_or_list(prepared_input),\n\u001b[32m    487\u001b[39m             context_wrapper,\n\u001b[32m    488\u001b[39m         ),\n\u001b[32m    489\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    490\u001b[39m             agent=current_agent,\n\u001b[32m    491\u001b[39m             all_tools=all_tools,\n\u001b[32m    492\u001b[39m             original_input=original_input,\n\u001b[32m    493\u001b[39m             generated_items=generated_items,\n\u001b[32m    494\u001b[39m             hooks=hooks,\n\u001b[32m    495\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    496\u001b[39m             run_config=run_config,\n\u001b[32m    497\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    498\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    499\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    500\u001b[39m             conversation_id=conversation_id,\n\u001b[32m    501\u001b[39m         ),\n\u001b[32m    502\u001b[39m     )\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    504\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    505\u001b[39m         agent=current_agent,\n\u001b[32m    506\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    515\u001b[39m         conversation_id=conversation_id,\n\u001b[32m    516\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agents/.venv/lib/python3.12/site-packages/agents/run.py:1146\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id, conversation_id)\u001b[39m\n\u001b[32m   1143\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m   1144\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m-> \u001b[39m\u001b[32m1146\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m   1147\u001b[39m     agent,\n\u001b[32m   1148\u001b[39m     system_prompt,\n\u001b[32m   1149\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1150\u001b[39m     output_schema,\n\u001b[32m   1151\u001b[39m     all_tools,\n\u001b[32m   1152\u001b[39m     handoffs,\n\u001b[32m   1153\u001b[39m     context_wrapper,\n\u001b[32m   1154\u001b[39m     run_config,\n\u001b[32m   1155\u001b[39m     tool_use_tracker,\n\u001b[32m   1156\u001b[39m     previous_response_id,\n\u001b[32m   1157\u001b[39m     conversation_id,\n\u001b[32m   1158\u001b[39m     prompt_config,\n\u001b[32m   1159\u001b[39m )\n\u001b[32m   1161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m   1162\u001b[39m     agent=agent,\n\u001b[32m   1163\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1172\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m   1173\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agents/.venv/lib/python3.12/site-packages/agents/run.py:1376\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id, conversation_id, prompt_config)\u001b[39m\n\u001b[32m   1368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m agent.hooks:\n\u001b[32m   1369\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m agent.hooks.on_llm_start(\n\u001b[32m   1370\u001b[39m         context_wrapper,\n\u001b[32m   1371\u001b[39m         agent,\n\u001b[32m   1372\u001b[39m         filtered.instructions,  \u001b[38;5;66;03m# Use filtered instructions\u001b[39;00m\n\u001b[32m   1373\u001b[39m         filtered.input,  \u001b[38;5;66;03m# Use filtered input\u001b[39;00m\n\u001b[32m   1374\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1377\u001b[39m     system_instructions=filtered.instructions,\n\u001b[32m   1378\u001b[39m     \u001b[38;5;28minput\u001b[39m=filtered.input,\n\u001b[32m   1379\u001b[39m     model_settings=model_settings,\n\u001b[32m   1380\u001b[39m     tools=all_tools,\n\u001b[32m   1381\u001b[39m     output_schema=output_schema,\n\u001b[32m   1382\u001b[39m     handoffs=handoffs,\n\u001b[32m   1383\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1384\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1385\u001b[39m     ),\n\u001b[32m   1386\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1387\u001b[39m     conversation_id=conversation_id,\n\u001b[32m   1388\u001b[39m     prompt=prompt_config,\n\u001b[32m   1389\u001b[39m )\n\u001b[32m   1390\u001b[39m \u001b[38;5;66;03m# If the agent has hooks, we need to call them after the LLM call\u001b[39;00m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m agent.hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agents/.venv/lib/python3.12/site-packages/agents/models/openai_chatcompletions.py:67\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, conversation_id, prompt)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     51\u001b[39m     system_instructions: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m     prompt: ResponsePromptParam | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     61\u001b[39m ) -> ModelResponse:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[32m     63\u001b[39m         model=\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model),\n\u001b[32m     64\u001b[39m         model_config=model_settings.to_json_dict() | {\u001b[33m\"\u001b[39m\u001b[33mbase_url\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m._client.base_url)},\n\u001b[32m     65\u001b[39m         disabled=tracing.is_disabled(),\n\u001b[32m     66\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     68\u001b[39m             system_instructions,\n\u001b[32m     69\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     70\u001b[39m             model_settings,\n\u001b[32m     71\u001b[39m             tools,\n\u001b[32m     72\u001b[39m             output_schema,\n\u001b[32m     73\u001b[39m             handoffs,\n\u001b[32m     74\u001b[39m             span_generation,\n\u001b[32m     75\u001b[39m             tracing,\n\u001b[32m     76\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     77\u001b[39m             prompt=prompt,\n\u001b[32m     78\u001b[39m         )\n\u001b[32m     80\u001b[39m         message: ChatCompletionMessage | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     81\u001b[39m         first_choice: Choice | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agents/.venv/lib/python3.12/site-packages/agents/models/openai_chatcompletions.py:276\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream, prompt)\u001b[39m\n\u001b[32m    270\u001b[39m store = ChatCmplHelpers.get_store_param(\u001b[38;5;28mself\u001b[39m._get_client(), model_settings)\n\u001b[32m    272\u001b[39m stream_options = ChatCmplHelpers.get_stream_options_param(\n\u001b[32m    273\u001b[39m     \u001b[38;5;28mself\u001b[39m._get_client(), model_settings, stream=stream\n\u001b[32m    274\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_client().chat.completions.create(\n\u001b[32m    277\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    278\u001b[39m     messages=converted_messages,\n\u001b[32m    279\u001b[39m     tools=converted_tools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    280\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    281\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    282\u001b[39m     frequency_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.frequency_penalty),\n\u001b[32m    283\u001b[39m     presence_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.presence_penalty),\n\u001b[32m    284\u001b[39m     max_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    285\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    286\u001b[39m     response_format=response_format,\n\u001b[32m    287\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    288\u001b[39m     stream=stream,\n\u001b[32m    289\u001b[39m     stream_options=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(stream_options),\n\u001b[32m    290\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(store),\n\u001b[32m    291\u001b[39m     reasoning_effort=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(reasoning_effort),\n\u001b[32m    292\u001b[39m     verbosity=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.verbosity),\n\u001b[32m    293\u001b[39m     top_logprobs=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_logprobs),\n\u001b[32m    294\u001b[39m     extra_headers={**HEADERS, **(model_settings.extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[32m    295\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    296\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    297\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    298\u001b[39m     **(model_settings.extra_args \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    299\u001b[39m )\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, ChatCompletion):\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agents/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:2583\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2537\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2538\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2539\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2580\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2581\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2582\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2583\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2584\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2585\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2586\u001b[39m             {\n\u001b[32m   2587\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2589\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2590\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2591\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2592\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2593\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2594\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2595\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2596\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2597\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2598\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2599\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2600\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2601\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2602\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2603\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2604\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2605\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2606\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2607\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2608\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2609\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2610\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2611\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2612\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2613\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2614\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2615\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2616\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2617\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2618\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2619\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2620\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2621\u001b[39m             },\n\u001b[32m   2622\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2623\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2624\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2625\u001b[39m         ),\n\u001b[32m   2626\u001b[39m         options=make_request_options(\n\u001b[32m   2627\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2628\u001b[39m         ),\n\u001b[32m   2629\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2630\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2631\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2632\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agents/.venv/lib/python3.12/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/agents/.venv/lib/python3.12/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1591\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1593\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"tool call validation failed: attempted to call tool 'Translator Tool_args' which was not in request.tools\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=Translator Tool_args>{\"input\": \"Hello, how are you?\"}</function>'}}"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(controller_agent, \"Can you help me solve 2x + 3 = 7\")\n",
    "print(result.last_agent.name)\n",
    "print(result.final_output)\n",
    "result = await Runner.run(controller_agent, \"i need a car, let's discuss\")\n",
    "print(result.last_agent.name)\n",
    "print(result.final_output)\n",
    "result = await Runner.run(controller_agent, \"Can you translate 'Hello, how are you?' to hindi?\")\n",
    "print(result.last_agent.name)\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
