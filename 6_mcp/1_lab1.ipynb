{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WELCOME TO WEEK 6\n",
    "\n",
    "The Epic Finale Week\n",
    "\n",
    "And\n",
    "\n",
    "# WELCOME TO THE **M**ODEL **C**ONTEXT **P**ROTOCOL!\n",
    "\n",
    "And welcome back to OpenAI Agents SDK ❤️❤️❤️\n",
    "\n",
    "### Please note\n",
    "\n",
    "There may be changes here from the video as I'm always making updates!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">To my Windows PC people - an important announcement</h2>\n",
    "            <span style=\"color:#ff7800;\">I have unpleasant news. There's a problem running MCP Servers on Windows PCs; Mac and Linux is fine. This is a known issue as of May 4th, 2025. I asked o3 with Deep Research to try to find workarounds; it <a href=\"https://chatgpt.com/share/6817bbc3-3d0c-8012-9b51-631842470628\">confirmed the issue</a> and confirmed the workaround.<br/><br/>\n",
    "            The workaround is a bit of a bore. It is to take advantage of \"WSL\", the Microsoft approach for running Linux on your PC. You'll need to carry out more setup instructions! But it's quick, and several students have confirmed that this works perfectly for them, then the Week 6 MCP labs work. Plus, WSL is actually a great way to build software on your Windows PC.<br/>\n",
    "            The WSL Setup instructions are in the Setup folder, <a href=\"../setup/SETUP-WSL.md\">in the file called SETUP-WSL.md here</a>. I do hope this only holds you up briefly - you should be back up and running quickly. Oh the joys of working with bleeding-edge technology!<br/><br/>\n",
    "            With many thanks to students Markus, Abhi, Hui-Ling, and several others, for helping me work on it and confirming the fix.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The imports\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, trace\n",
    "from agents.mcp import MCPServerStdio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, trace, function_tool, OpenAIChatCompletionsModel\n",
    "import os\n",
    "from sendgrid.helpers.mail import Mail, Email, To, Content\n",
    "from openai import AsyncOpenAI\n",
    "load_dotenv(override=True)\n",
    "\n",
    "MODEL = 'llama-3.3-70b-versatile'\n",
    "GROQ_BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")  # Add your Groq API key to your .env file\n",
    "\n",
    "# Point OpenAI client to Groq endpoint\n",
    "groq_client = AsyncOpenAI(\n",
    "    api_key=groq_api_key,\n",
    "    base_url=GROQ_BASE_URL\n",
    ")\n",
    "\n",
    "groq_model = OpenAIChatCompletionsModel(model=MODEL, openai_client=groq_client)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now.. bring on the Agent with Tools!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shivang.gupta/Documents/GitHub/agents/6_mcp/sandbox\n",
      "The function calls were executed successfully. Here is the recipe for banoffee, summarized in markdown and written to the file banoffee.md:\n",
      "\n",
      "# Banoffee Recipe\n",
      "## Ingredients:\n",
      "* 1 cup granulated sugar\n",
      "* 1/2 cup light brown soft sugar\n",
      "* 1/2 cup all-purpose flour\n",
      "* 1/2 teaspoon baking powder\n",
      "* 1/4 teaspoon salt\n",
      "* 1/2 cup unsalted butter, melted\n",
      "* 2 large eggs\n",
      "* 2 teaspoons pure vanilla extract\n",
      "* 1 cup heavy cream\n",
      "* 1 cup sliced bananas\n",
      "* 1 cup toffee bits\n",
      "* 1 cup chopped walnuts (optional)\n",
      "\n",
      "## Instructions:\n",
      "1. Preheat oven to 350°F (180°C).\n",
      "2. Line an 8-inch round cake pan with parchment paper.\n",
      "3. In a medium bowl, whisk together flour, baking powder, and salt.\n",
      "4. In a large bowl, use an electric mixer to beat together sugars and melted butter until well combined.\n",
      "5. Beat in eggs one at a time, followed by vanilla extract.\n",
      "6. Gradually mix in the flour mixture, then stir in heavy cream.\n",
      "7. Fold in sliced bananas, toffee bits, and chopped walnuts (if using).\n",
      "8. Pour batter into prepared cake pan and smooth top.\n",
      "9. Bake for 35-40 minutes or until a toothpick inserted into the center comes out clean.\n",
      "10. Remove from oven and let cool in pan for 10 minutes, then transfer to a wire rack to cool completely.\n"
     ]
    }
   ],
   "source": [
    "instructions = \"\"\"\n",
    "You browse the internet to accomplish your instructions.\n",
    "You are highly capable at browsing the internet independently to accomplish your task, \n",
    "including accepting all cookies\n",
    "\"\"\"\n",
    "\n",
    "sandbox_path = os.path.abspath(os.path.join(os.getcwd(), \"sandbox\"))\n",
    "print(sandbox_path)\n",
    "files_params = {\"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", sandbox_path]}\n",
    "playwright_params = {\"command\": \"uvx\",\"args\": [ \"mcp-server-browser\"]}\n",
    "\n",
    "async with MCPServerStdio(params=files_params, client_session_timeout_seconds=600) as mcp_server_files:\n",
    "    async with MCPServerStdio(params=playwright_params, client_session_timeout_seconds=600) as mcp_server_browser:\n",
    "        agent = Agent(\n",
    "            name=\"investigator\", \n",
    "            instructions=instructions, \n",
    "            model=groq_model,\n",
    "            mcp_servers=[mcp_server_files, mcp_server_browser]\n",
    "            )\n",
    "        with trace(\"investigate\"):\n",
    "            result = await Runner.run(agent, \"Get the recipe for banoffee, then summarize it in markdown to banoffee.md\")\n",
    "            print(result.final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out the trace\n",
    "\n",
    "https://platform.openai.com/traces\n",
    "\n",
    "### Now take a look at some MCP marketplaces\n",
    "\n",
    "https://mcp.so\n",
    "\n",
    "https://glama.ai/mcp\n",
    "\n",
    "https://smithery.ai/\n",
    "\n",
    "https://huggingface.co/blog/LLMhacker/top-11-essential-mcp-libraries\n",
    "\n",
    "HuggingFace great community article:\n",
    "https://huggingface.co/blog/Kseniase/mcp\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
